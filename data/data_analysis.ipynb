{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‰∏≠ÊñáÊï∞ÊçÆÂàÜÊûê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,AutoModelForSeq2SeqLM\n",
    "pretrained_model = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/pretrained-model/zh/BART/'\n",
    "zh_Tokenizer=BertTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.405843561091775 17.405843561091775 45\n",
      "15.405843561091775 45\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/AISHELL-1/AISHELL-1_train.txt'\n",
    "with open(file_path, 'r') as f_data:\n",
    "    data = f_data.readlines()\n",
    "    text = [item.strip().split(' ')[1] for item in data]\n",
    "    token = [zh_Tokenizer(item) for item in text]\n",
    "    token_lengths = [len(item['input_ids']) for item in token]\n",
    "    lengths = [len(item) for item in text]\n",
    "    token_mean_lengths = sum(token_lengths)/len(token_lengths)\n",
    "    mean_lengths = sum(lengths)/len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    print(mean_lengths, token_mean_lengths,max_length)\n",
    "    print(mean_lengths, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.sort(reverse=True)\n",
    "lengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.89748643158182 12.837930929929353 44\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/AIDATATANG/AIDATATANG_train.txt'\n",
    "with open(file_path, 'r') as f_data:\n",
    "    data = f_data.readlines()\n",
    "    text = [item.strip().split(' ')[1] for item in data]\n",
    "    token = [zh_Tokenizer(item) for item in text]\n",
    "    token_lengths = [len(item['input_ids']) for item in token]\n",
    "    lengths = [len(item) for item in text]\n",
    "    token_mean_lengths = sum(token_lengths)/len(token_lengths)\n",
    "    mean_lengths = sum(lengths)/len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    print(mean_lengths, token_mean_lengths,max_length)\n",
    "    print(mean_lengths, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.887101161683622 12.882964996041697 73\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/MAGICDATA/MAGICDATA_train.txt'\n",
    "with open(file_path, 'r') as f_data:\n",
    "    data = f_data.readlines()\n",
    "    text = [item.strip().split(' ')[1] for item in data]\n",
    "    token = [zh_Tokenizer(item) for item in text]\n",
    "    token_lengths = [len(item['input_ids']) for item in token]\n",
    "    lengths = [len(item) for item in text]\n",
    "    token_mean_lengths = sum(token_lengths)/len(token_lengths)\n",
    "    mean_lengths = sum(lengths)/len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    print(mean_lengths, token_mean_lengths,max_length)\n",
    "    print(mean_lengths, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ëã±ÊñáÊï∞ÊçÆÂàÜÊûê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "pretrained_model = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/pretrained-model/en/BART'\n",
    "en_Tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.59444901284769 41.123626021289596 83\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/en/LIBRISPEECH_CLEAN/LIBRISPEECH_CLEAN_train.txt'\n",
    "with open(file_path, 'r') as f_data:\n",
    "    data = f_data.readlines()\n",
    "    label = [item.strip().split('|')[1] for item in data]\n",
    "    text = [item.strip().split('|')[2] for item in data]\n",
    "    token = [en_Tokenizer(item) for item in label]\n",
    "    token_lengths = [len(item['input_ids']) for item in token]\n",
    "    lengths = [len(item.split(' ')) for item in label]\n",
    "    token_mean_lengths = sum(token_lengths)/len(token_lengths)\n",
    "    mean_lengths = sum(lengths)/len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    print(mean_lengths, token_mean_lengths,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6131/3282927130.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('wer')\n",
      "Using the latest cached version of the module from /home/users/jiangjin/.cache/huggingface/modules/datasets_modules/metrics/wer/d435ee8ec9ba888aa5e23b481ea6ce4e88ca1384d5bda6f0528a6dea387e368d (last modified on Tue Dec  7 16:32:47 2021) since it couldn't be found locally at wer, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('wer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"and sail for the north seas day after he turned away and began to whistle as if he did not wish for any further conversation with his interrogator philip indeed had nothing more to say to him he had learned all he wanted to know i'd like to bid good by to sylvie is she at home he asked of her father.\",\n",
       " \"and sail for the north season's day he turned away and began to whistle as if he did not wish for any further conversation with his interrogator philip indeed had nothing more to say to him he had learned all he wanted to know he was a bit good by to sylvie is she at home he asked of her father.\",\n",
       " 0.0967741935483871)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 100\n",
    "value = metric.compute(references=[label[index]], predictions=[text[index]])\n",
    "label[index],text[index],value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‰∏≠Êñáraw dataÁªìÊûúÁöÑÁ°ÆËÆ§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AISHELL-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28254/3098461382.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('cer')\n",
      "Using the latest cached version of the module from /home/users/jiangjin/.cache/huggingface/modules/datasets_modules/metrics/cer/0d603b79fde740594c09751048122254b33a79b1c45328bd72ca604534ce8156 (last modified on Tue Dec 14 16:32:40 2021) since it couldn't be found locally at cer, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('cer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,AutoModelForSeq2SeqLM\n",
    "pretrained_model = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/pretrained-model/zh/BART/'\n",
    "zh_Tokenizer=BertTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/AISHELL-1/AISHELL-1_test.txt'\n",
    "with open(file_path, 'r') as f_zh_data:\n",
    "    data = f_zh_data.readlines()\n",
    "    utt = [item.split(' ')[0] for item in data]\n",
    "    label = [item.split(' ')[2] for item in data]\n",
    "    record = [item.split(' ')[1] for item in data]\n",
    "metric_value = metric.compute(references=label,predictions=record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04308167400016065"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ÁîöËá≥Âá∫Áé∞‰∫§ÊòìÂá†‰πéÂÅúÊªûÁöÑÊÉÖÂÜµ„ÄÇ\\n',\n",
       "  '‰∏Ä‰∫åÁ∫øÂüéÂ∏ÇËôΩÁÑ∂‰πüÂ§Ñ‰∫éË∞ÉÊï¥‰∏≠„ÄÇ\\n',\n",
       "  'Â§ßÂõ†‰∏∫ËÅöÈõÜ‰∫ÜËøáÂ§öÂÖ¨ÂÖ±ËµÑÊ∫ê„ÄÇ\\n',\n",
       "  '‰∏∫‰∫ÜËßÑÈÅø‰∏âÂõõÁ∫øÂüéÂ∏ÇÊòéÊòæËøáÂâ©ÁöÑÂ∏ÇÂú∫È£éÈô©„ÄÇ\\n',\n",
       "  'Ê†áÊùÜÊàø‰ºÅÂøÖÁÑ∂Ë∞ÉÊï¥Â∏ÇÂú∫ÊàòÁï•„ÄÇ\\n',\n",
       "  'Âõ†Ê≠§ÂúüÂú∞ÂÇ®Â§áËá≥ÂÖ≥ÈáçË¶Å„ÄÇ\\n',\n",
       "  '‰∏≠ÂéüÂú∞‰∫ßÈ¶ñÂ∏≠ÂàÜÊûêÂ∏àÂº†Â§ß‰ºüËØ¥„ÄÇ\\n',\n",
       "  '‰∏ÄÁ∫øÂüéÂ∏ÇÂúüÂú∞‰æõÂ∫îÈáèÂáèÂ∞ë„ÄÇ\\n',\n",
       "  '‰πüÂä©Êé®‰∫ÜÂúüÂú∞Â∏ÇÂú∫ÁöÑÁÅ´ÁàÜ„ÄÇ\\n',\n",
       "  'Âåó‰∫¨ÂáèËñ™ÂÉß‰ΩèÂÆÖÂúüÂú∞‰æõÂ∫îÊó∂Êùæ„ÄÇ\\n'],\n",
       " ['ÁîöËá≥Âá∫Áé∞‰∫§ÊòìÂá†‰πéÂÅúÊªûÁöÑÊÉÖÂÜµ„ÄÇ',\n",
       "  '‰∏Ä‰∫åÁ∫øÂüéÂ∏ÇËôΩÁÑ∂‰πüÂ§Ñ‰∫éË∞ÉÊï¥‰∏≠„ÄÇ',\n",
       "  '‰ΩÜÂõ†‰∏∫ËÅöÈõÜ‰∫ÜËøáÂ§öÂÖ¨ÂÖ±ËµÑÊ∫ê„ÄÇ',\n",
       "  '‰∏∫‰∫ÜËßÑÈÅø‰∏âÂõõÁ∫øÂüéÂ∏ÇÊòéÊòæËøáÂâ©ÁöÑÂ∏ÇÂú∫È£éÈô©„ÄÇ',\n",
       "  'Ê†áÊùÜÊàø‰ºÅÂøÖÁÑ∂Ë∞ÉÊï¥Â∏ÇÂú∫ÊàòÁï•„ÄÇ',\n",
       "  'Âõ†Ê≠§ÂúüÂú∞ÂÇ®Â§áËá≥ÂÖ≥ÈáçË¶Å„ÄÇ',\n",
       "  '‰∏≠ÂéüÂú∞‰∫ßÈ¶ñÂ∏≠ÂàÜÊûêÂ∏àÂº†Â§ß‰ºüËØ¥„ÄÇ',\n",
       "  '‰∏ÄÁ∫øÂüéÂ∏ÇÂúüÂú∞‰æõÂ∫îÈáèÂáèÂ∞ë„ÄÇ',\n",
       "  '‰πüÂä©Êé®‰∫ÜÂúüÂú∞Â∏ÇÂú∫ÁöÑÁÅ´ÁàÜ„ÄÇ',\n",
       "  'Âåó‰∫¨‰ªÖÊñ∞Â¢û‰ΩèÂÆÖÂúüÂú∞‰æõÂ∫îÂçÅÂÆó„ÄÇ'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0:10], record[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = zh_Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = text_tokenizer.batch_encode_plus(\n",
    "            label,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n",
    "record_ids = text_tokenizer.batch_encode_plus(\n",
    "            record,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7176, 40])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_ids['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label = text_tokenizer.batch_decode(\n",
    "                    label_ids['input_ids'], skip_special_tokens=True)\n",
    "decode_record = text_tokenizer.batch_decode(\n",
    "                    record_ids['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label  = [item.replace(' ', '') for item in decode_label]\n",
    "decode_record  = [item.replace(' ', '') for item in decode_record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_value = metric.compute(references=decode_label,predictions=decode_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04308167400016065"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value# ÂíåËøôÈáåÁöÑÂæÆÂ∞èÂ∑ÆË∑ùÂèØËÉΩÂú®Ôºå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÁîöËá≥Âá∫Áé∞‰∫§ÊòìÂá†‰πéÂÅúÊªûÁöÑÊÉÖÂÜµ„ÄÇ',\n",
       " '‰∏Ä‰∫åÁ∫øÂüéÂ∏ÇËôΩÁÑ∂‰πüÂ§Ñ‰∫éË∞ÉÊï¥‰∏≠„ÄÇ',\n",
       " 'Â§ßÂõ†‰∏∫ËÅöÈõÜ‰∫ÜËøáÂ§öÂÖ¨ÂÖ±ËµÑÊ∫ê„ÄÇ',\n",
       " '‰∏∫‰∫ÜËßÑÈÅø‰∏âÂõõÁ∫øÂüéÂ∏ÇÊòéÊòæËøáÂâ©ÁöÑÂ∏ÇÂú∫È£éÈô©„ÄÇ',\n",
       " 'Ê†áÊùÜÊàø‰ºÅÂøÖÁÑ∂Ë∞ÉÊï¥Â∏ÇÂú∫ÊàòÁï•„ÄÇ',\n",
       " 'Âõ†Ê≠§ÂúüÂú∞ÂÇ®Â§áËá≥ÂÖ≥ÈáçË¶Å„ÄÇ',\n",
       " '‰∏≠ÂéüÂú∞‰∫ßÈ¶ñÂ∏≠ÂàÜÊûêÂ∏àÂº†Â§ß‰ºüËØ¥„ÄÇ',\n",
       " '‰∏ÄÁ∫øÂüéÂ∏ÇÂúüÂú∞‰æõÂ∫îÈáèÂáèÂ∞ë„ÄÇ',\n",
       " '‰πüÂä©Êé®‰∫ÜÂúüÂú∞Â∏ÇÂú∫ÁöÑÁÅ´ÁàÜ„ÄÇ',\n",
       " 'Âåó‰∫¨ÂáèËñ™ÂÉß‰ΩèÂÆÖÂúüÂú∞‰æõÂ∫îÊó∂Êùæ„ÄÇ']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_label[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIDATATANG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/AIDATATANG/AIDATATANG_test.txt'\n",
    "with open(file_path, 'r') as f_zh_data:\n",
    "    data = f_zh_data.readlines()\n",
    "    utt = [item.split(' ')[0] for item in data]\n",
    "    label = [item.split(' ')[2] for item in data]\n",
    "    record = [item.split(' ')[1] for item in data]\n",
    "metric_value = metric.compute(references=label,predictions=record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06674709801510594"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Êú∫Á•®ÂπøÂ∑ûÊµéÂçó„ÄÇ\\n',\n",
       "  'Á¥¢È©¨ÈáåÊµ∑ÂüüÂú∞ÁêÜ‰ΩçÁΩÆÈáçË¶ÅÊÄß„ÄÇ\\n',\n",
       "  'Êâæ‰∏™ÊøÄÊÉÖÁâáÁúã‰∏ÄÁúã„ÄÇ\\n',\n",
       "  'Êù•‰∏ÄÈÉ®Â•ΩÁúãÁöÑÁîµÂΩ±„ÄÇ\\n',\n",
       "  'ÊàëË¶ÅÊãçÊëÑ‰∫åÁª¥Á†Å„ÄÇ\\n',\n",
       "  'Êí≠ÊîæÂéüÊù•‰Ω†‰ªÄ‰πàÈÉΩ‰∏çÊÉ≥Ë¶Å„ÄÇ\\n',\n",
       "  '‰∏ÄÊúàÂçÅ‰∏ÄÊó•‰∏äÂçàÂçÅÂÖ´ÁÇπÊèêÈÜíÊàë„ÄÇ\\n',\n",
       "  '‰Ω†‰∏çÊòØË¶ÅÁªôÊàëËÆ≤‰∏™Á¨ëËØùÂêó„ÄÇ\\n',\n",
       "  'ÂÆ∂ÈáåÈù¢Êúâ‰∏Ä‰∏™Â∞èÁôΩÂÖî„ÄÇ\\n',\n",
       "  'ÊòéÂ§©ÊúâÊ≤°ÊúâÈõ®Âêó„ÄÇ\\n'],\n",
       " ['Êú∫Á•®ÂπøÂ∑ûÊµéÂçó„ÄÇ',\n",
       "  'Á¥¢È©¨ÈáåÊµ∑ÂüüÂú∞ÁêÜ‰ΩçÁΩÆÈáçË¶ÅÊÄß„ÄÇ',\n",
       "  'Êâæ‰∏™ÊøÄÊÉÖÁâáÁúã‰∏ÄÁúã„ÄÇ',\n",
       "  'Êù•‰∏ÄÈÉ®Â•ΩÁúãÁöÑÁîµÂΩ±„ÄÇ',\n",
       "  'ÊàëË¶ÅÊãçÊëÑ‰∫åÁª¥Á†Å„ÄÇ',\n",
       "  'Êí≠ÊîæÂéüÊù•‰Ω†‰ªÄ‰πàÈÉΩ‰∏çÊÉ≥Ë¶Å„ÄÇ',\n",
       "  '‰∏ÄÊúàÂçÅ‰∏ÄÊó•‰∏äÂçàÂçÅÂÖ´ÁÇπÊèêÈÜíÊàë„ÄÇ',\n",
       "  '‰Ω†‰∏çÊòØË¶ÅÁªôÊàëËÆ≤‰∏™Á¨ëËØùÂêó„ÄÇ',\n",
       "  'ÂÆ∂ÈáåÈù¢Êúâ‰∏Ä‰∏™Â∞èÁôΩÂÖî„ÄÇ',\n",
       "  'ÊòéÂ§©ÊúâÊú®ÊúâÈõ®Âêó„ÄÇ'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0:10], record[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = zh_Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = text_tokenizer.batch_encode_plus(\n",
    "            label,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n",
    "record_ids = text_tokenizer.batch_encode_plus(\n",
    "            record,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48144, 40])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_ids['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label = text_tokenizer.batch_decode(\n",
    "                    label_ids['input_ids'], skip_special_tokens=True)\n",
    "decode_record = text_tokenizer.batch_decode(\n",
    "                    record_ids['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label  = [item.replace(' ', '') for item in decode_label]\n",
    "decode_record  = [item.replace(' ', '') for item in decode_record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_value = metric.compute(references=decode_label,predictions=decode_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04308167400016065"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value# ÂíåËøôÈáåÁöÑÂæÆÂ∞èÂ∑ÆË∑ùÂèØËÉΩÂú®Ôºå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÁîöËá≥Âá∫Áé∞‰∫§ÊòìÂá†‰πéÂÅúÊªûÁöÑÊÉÖÂÜµ„ÄÇ',\n",
       " '‰∏Ä‰∫åÁ∫øÂüéÂ∏ÇËôΩÁÑ∂‰πüÂ§Ñ‰∫éË∞ÉÊï¥‰∏≠„ÄÇ',\n",
       " 'Â§ßÂõ†‰∏∫ËÅöÈõÜ‰∫ÜËøáÂ§öÂÖ¨ÂÖ±ËµÑÊ∫ê„ÄÇ',\n",
       " '‰∏∫‰∫ÜËßÑÈÅø‰∏âÂõõÁ∫øÂüéÂ∏ÇÊòéÊòæËøáÂâ©ÁöÑÂ∏ÇÂú∫È£éÈô©„ÄÇ',\n",
       " 'Ê†áÊùÜÊàø‰ºÅÂøÖÁÑ∂Ë∞ÉÊï¥Â∏ÇÂú∫ÊàòÁï•„ÄÇ',\n",
       " 'Âõ†Ê≠§ÂúüÂú∞ÂÇ®Â§áËá≥ÂÖ≥ÈáçË¶Å„ÄÇ',\n",
       " '‰∏≠ÂéüÂú∞‰∫ßÈ¶ñÂ∏≠ÂàÜÊûêÂ∏àÂº†Â§ß‰ºüËØ¥„ÄÇ',\n",
       " '‰∏ÄÁ∫øÂüéÂ∏ÇÂúüÂú∞‰æõÂ∫îÈáèÂáèÂ∞ë„ÄÇ',\n",
       " '‰πüÂä©Êé®‰∫ÜÂúüÂú∞Â∏ÇÂú∫ÁöÑÁÅ´ÁàÜ„ÄÇ',\n",
       " 'Âåó‰∫¨ÂáèËñ™ÂÉß‰ΩèÂÆÖÂúüÂú∞‰æõÂ∫îÊó∂Êùæ„ÄÇ']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAGICDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/MAGICDATA/MAGICDATA_test.txt'\n",
    "with open(file_path, 'r') as f_zh_data:\n",
    "    data = f_zh_data.readlines()\n",
    "    utt = [item.split(' ')[0] for item in data]\n",
    "    label = [item.split(' ')[2] for item in data]\n",
    "    record = [item.split(' ')[1] for item in data]\n",
    "metric_value = metric.compute(references=label,predictions=record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07398781866368245"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Âè£Âè£Èü≥‰πê„ÄÇ\\n',\n",
       "  'Êµ∑Â§©Ê∞îÊµ∑ÂÜ∑ËÆ∞ÂæóÂ§©Ë°£‰øùÊöñÂì¶„ÄÇ\\n',\n",
       "  '‰∫∫ÁöÑËÉΩËÄêÂÜçÂ§ßÂ§ß‰∏çËøáÂ§©ËøôÈõ®ËΩ¨Èò¥ÂèàÂèò‰∫ÜÂ§©ËÄÅÂ§©Ë¶Å‰∏ãÈõ®Â®òË¶ÅÂ´Å‰∫∫Áî±Â•πÂéªÂêß„ÄÇ\\n',\n",
       "  'Êí≠Êîæ‰∫åËÉ°Áã¨Â•è„ÄÇ\\n',\n",
       "  'ÂèØÊòØÂàÆÈ£é‰∫Ü„ÄÇ\\n',\n",
       "  'ÂçóÊ¥ãÊåÅÁª≠ÈôçÈõ®Ê∞¥Ê∑±Ë∑ØÊªëÂá∫Ë°å‰∏ÄÂÆöË¶ÅÊ≥®ÊÑèÂüÉ‰∏îÂàöÂõûÂÆ∂ÁöÑË∑Ø‰∏äÊàëÂ∑ÆÁÇπË¢´ÂÜ≤Ëøõ‰∏ãÊ∞¥ÈÅì„ÄÇ\\n',\n",
       "  'Êí≠ÊîæÈõ™ËéâÁöÑÊ≠åÊõ≤„ÄÇ\\n',\n",
       "  'ÊàëÊÉ≥ËøôÂá∫ÊúÄÂ•ΩÊ≠åÊõ≤ÊääÊ≠åËØçÂèëÂà∞ÁΩë‰∏äËØ∑Âà´‰∫∫Â∏ÆÊàë‰ΩúÊõ≤Êé•Êú∫„ÄÇ\\n',\n",
       "  'Âà´ËØ¥‰∏™Â§ßÊ¶ÇÈΩêËØ¥ÂÖ∑‰ΩìÊòØÂì™È¶ñÊ≠å„ÄÇ\\n',\n",
       "  'Èô§‰∫ÜËá™ÂëΩ‰∏çÁôΩÂíåÁà±ÊòØ‰∏ÄÁßçÂπ∏Á¶è„ÄÇ\\n'],\n",
       " ['Âè£Âè£Èü≥‰πê„ÄÇ',\n",
       "  'Âó®Â§©Ê∞îÂØíÂÜ∑ËÆ∞ÂæóÊ∑ªË°£‰øùÊöñÂì¶„ÄÇ',\n",
       "  '‰∫∫ÁöÑËÉΩËÄêÂÜçÂ§ßÂ§ß‰∏çËøáÂ§©ÈòµÈõ®ËΩ¨Èò¥ÂèàÂèò‰∫ÜÂ§©‰∫ÜÂ§©Ë¶Å‰∏ãÈõ®Â®òË¶ÅÂ´Å‰∫∫Áî±Â•πÂéªÂêß„ÄÇ',\n",
       "  'Êí≠Êîæ‰∫åËÉ°Áã¨Â•è„ÄÇ',\n",
       "  'ÂèØÊòØÂàÆÈ£é‰∫Ü„ÄÇ',\n",
       "  'ÂçóÈò≥ÊåÅÁª≠ÈôçÈõ®Ê∞¥Ê∑±Ë∑ØÊªëÂá∫Ë°å‰∏ÄÂÆöË¶ÅÊ≥®ÊÑèÂÆâÂÖ®ÂàöÂõûÂÆ∂ÁöÑË∑Ø‰∏äÊàëÂ∑ÆÁÇπÂÑøË¢´ÂÜ≤Ëøõ‰∏ãÊ∞¥ÈÅì„ÄÇ',\n",
       "  'Êí≠ÊîæÈõ™ËéâÁöÑÊ≠åÊõ≤„ÄÇ',\n",
       "  'ÊàëÊÉ≥ËøôÂá∫ÊúÄÂ•ΩÊ≠åÊõ≤ÊääÊ≠åËØçÂèëÂà∞ÁΩë‰∏äËØ∑Âà´‰∫∫Â∏ÆÊàë‰ΩúÊõ≤ÊÄ•ÊÄ•„ÄÇ',\n",
       "  'Âà´ËØ¥‰∏™Â§ßÊ¶ÇÈΩêËØ¥ÂÖ∑‰ΩìÊòØÂì™È¶ñÊ≠å„ÄÇ',\n",
       "  'Èô§‰∫ÜËá™ÂëΩ‰∏çÂá°ÂíåÁà±ÊòØ‰∏ÄÁßçÂπ∏Á¶è„ÄÇ'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0:10], record[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = zh_Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = text_tokenizer.batch_encode_plus(\n",
    "            label,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n",
    "record_ids = text_tokenizer.batch_encode_plus(\n",
    "            record,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24279, 74])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_ids['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label = text_tokenizer.batch_decode(\n",
    "                    label_ids['input_ids'], skip_special_tokens=True)\n",
    "decode_record = text_tokenizer.batch_decode(\n",
    "                    record_ids['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label  = [item.replace(' ', '') for item in decode_label]\n",
    "decode_record  = [item.replace(' ', '') for item in decode_record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_value = metric.compute(references=decode_label,predictions=decode_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07398837811577272"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value# ÂíåËøôÈáåÁöÑÂæÆÂ∞èÂ∑ÆË∑ùÂèØËÉΩÂú®Ôºå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Âè£Âè£Èü≥‰πê„ÄÇ',\n",
       " 'Êµ∑Â§©Ê∞îÊµ∑ÂÜ∑ËÆ∞ÂæóÂ§©Ë°£‰øùÊöñÂì¶„ÄÇ',\n",
       " '‰∫∫ÁöÑËÉΩËÄêÂÜçÂ§ßÂ§ß‰∏çËøáÂ§©ËøôÈõ®ËΩ¨Èò¥ÂèàÂèò‰∫ÜÂ§©ËÄÅÂ§©Ë¶Å‰∏ãÈõ®Â®òË¶ÅÂ´Å‰∫∫Áî±Â•πÂéªÂêß„ÄÇ',\n",
       " 'Êí≠Êîæ‰∫åËÉ°Áã¨Â•è„ÄÇ',\n",
       " 'ÂèØÊòØÂàÆÈ£é‰∫Ü„ÄÇ',\n",
       " 'ÂçóÊ¥ãÊåÅÁª≠ÈôçÈõ®Ê∞¥Ê∑±Ë∑ØÊªëÂá∫Ë°å‰∏ÄÂÆöË¶ÅÊ≥®ÊÑèÂüÉ‰∏îÂàöÂõûÂÆ∂ÁöÑË∑Ø‰∏äÊàëÂ∑ÆÁÇπË¢´ÂÜ≤Ëøõ‰∏ãÊ∞¥ÈÅì„ÄÇ',\n",
       " 'Êí≠ÊîæÈõ™ËéâÁöÑÊ≠åÊõ≤„ÄÇ',\n",
       " 'ÊàëÊÉ≥ËøôÂá∫ÊúÄÂ•ΩÊ≠åÊõ≤ÊääÊ≠åËØçÂèëÂà∞ÁΩë‰∏äËØ∑Âà´‰∫∫Â∏ÆÊàë‰ΩúÊõ≤Êé•Êú∫„ÄÇ',\n",
       " 'Âà´ËØ¥‰∏™Â§ßÊ¶ÇÈΩêËØ¥ÂÖ∑‰ΩìÊòØÂì™È¶ñÊ≠å„ÄÇ',\n",
       " 'Èô§‰∫ÜËá™ÂëΩ‰∏çÁôΩÂíåÁà±ÊòØ‰∏ÄÁßçÂπ∏Á¶è„ÄÇ']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3228b2b04fe574edc1cfad377da45b01c8bfafeb5d7448f83b83b0b5984135"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
