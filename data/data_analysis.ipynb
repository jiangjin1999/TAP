{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸­æ–‡æ•°æ®åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,AutoModelForSeq2SeqLM\n",
    "pretrained_model = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/pretrained-model/zh/BART/'\n",
    "zh_Tokenizer=BertTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.405843561091775 17.405843561091775 45\n",
      "15.405843561091775 45\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/AISHELL-1/AISHELL-1_train.txt'\n",
    "with open(file_path, 'r') as f_data:\n",
    "    data = f_data.readlines()\n",
    "    text = [item.strip().split(' ')[1] for item in data]\n",
    "    token = [zh_Tokenizer(item) for item in text]\n",
    "    token_lengths = [len(item['input_ids']) for item in token]\n",
    "    lengths = [len(item) for item in text]\n",
    "    token_mean_lengths = sum(token_lengths)/len(token_lengths)\n",
    "    mean_lengths = sum(lengths)/len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    print(mean_lengths, token_mean_lengths,max_length)\n",
    "    print(mean_lengths, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.sort(reverse=True)\n",
    "lengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.89748643158182 12.837930929929353 44\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/AIDATATANG/AIDATATANG_train.txt'\n",
    "with open(file_path, 'r') as f_data:\n",
    "    data = f_data.readlines()\n",
    "    text = [item.strip().split(' ')[1] for item in data]\n",
    "    token = [zh_Tokenizer(item) for item in text]\n",
    "    token_lengths = [len(item['input_ids']) for item in token]\n",
    "    lengths = [len(item) for item in text]\n",
    "    token_mean_lengths = sum(token_lengths)/len(token_lengths)\n",
    "    mean_lengths = sum(lengths)/len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    print(mean_lengths, token_mean_lengths,max_length)\n",
    "    print(mean_lengths, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.887101161683622 12.882964996041697 73\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/MAGICDATA/MAGICDATA_train.txt'\n",
    "with open(file_path, 'r') as f_data:\n",
    "    data = f_data.readlines()\n",
    "    text = [item.strip().split(' ')[1] for item in data]\n",
    "    token = [zh_Tokenizer(item) for item in text]\n",
    "    token_lengths = [len(item['input_ids']) for item in token]\n",
    "    lengths = [len(item) for item in text]\n",
    "    token_mean_lengths = sum(token_lengths)/len(token_lengths)\n",
    "    mean_lengths = sum(lengths)/len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    print(mean_lengths, token_mean_lengths,max_length)\n",
    "    print(mean_lengths, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‹±æ–‡æ•°æ®åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "pretrained_model = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/pretrained-model/en/BART'\n",
    "en_Tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.59444901284769 41.123626021289596 83\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/en/LIBRISPEECH_CLEAN/LIBRISPEECH_CLEAN_train.txt'\n",
    "with open(file_path, 'r') as f_data:\n",
    "    data = f_data.readlines()\n",
    "    label = [item.strip().split('|')[1] for item in data]\n",
    "    text = [item.strip().split('|')[2] for item in data]\n",
    "    token = [en_Tokenizer(item) for item in label]\n",
    "    token_lengths = [len(item['input_ids']) for item in token]\n",
    "    lengths = [len(item.split(' ')) for item in label]\n",
    "    token_mean_lengths = sum(token_lengths)/len(token_lengths)\n",
    "    mean_lengths = sum(lengths)/len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    print(mean_lengths, token_mean_lengths,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6131/3282927130.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('wer')\n",
      "Using the latest cached version of the module from /home/users/jiangjin/.cache/huggingface/modules/datasets_modules/metrics/wer/d435ee8ec9ba888aa5e23b481ea6ce4e88ca1384d5bda6f0528a6dea387e368d (last modified on Tue Dec  7 16:32:47 2021) since it couldn't be found locally at wer, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('wer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"and sail for the north seas day after he turned away and began to whistle as if he did not wish for any further conversation with his interrogator philip indeed had nothing more to say to him he had learned all he wanted to know i'd like to bid good by to sylvie is she at home he asked of her father.\",\n",
       " \"and sail for the north season's day he turned away and began to whistle as if he did not wish for any further conversation with his interrogator philip indeed had nothing more to say to him he had learned all he wanted to know he was a bit good by to sylvie is she at home he asked of her father.\",\n",
       " 0.0967741935483871)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 100\n",
    "value = metric.compute(references=[label[index]], predictions=[text[index]])\n",
    "label[index],text[index],value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸­æ–‡raw dataç»“æœçš„ç¡®è®¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AISHELL-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28254/3098461382.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('cer')\n",
      "Using the latest cached version of the module from /home/users/jiangjin/.cache/huggingface/modules/datasets_modules/metrics/cer/0d603b79fde740594c09751048122254b33a79b1c45328bd72ca604534ce8156 (last modified on Tue Dec 14 16:32:40 2021) since it couldn't be found locally at cer, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('cer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,AutoModelForSeq2SeqLM\n",
    "pretrained_model = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/pretrained-model/zh/BART/'\n",
    "zh_Tokenizer=BertTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/AISHELL-1/AISHELL-1_test.txt'\n",
    "with open(file_path, 'r') as f_zh_data:\n",
    "    data = f_zh_data.readlines()\n",
    "    utt = [item.split(' ')[0] for item in data]\n",
    "    label = [item.split(' ')[2] for item in data]\n",
    "    record = [item.split(' ')[1] for item in data]\n",
    "metric_value = metric.compute(references=label,predictions=record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04308167400016065"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ç”šè‡³å‡ºç°äº¤æ˜“å‡ ä¹åœæ»çš„æƒ…å†µã€‚\\n',\n",
       "  'ä¸€äºŒçº¿åŸå¸‚è™½ç„¶ä¹Ÿå¤„äºè°ƒæ•´ä¸­ã€‚\\n',\n",
       "  'å¤§å› ä¸ºèšé›†äº†è¿‡å¤šå…¬å…±èµ„æºã€‚\\n',\n",
       "  'ä¸ºäº†è§„é¿ä¸‰å››çº¿åŸå¸‚æ˜æ˜¾è¿‡å‰©çš„å¸‚åœºé£é™©ã€‚\\n',\n",
       "  'æ ‡æ†æˆ¿ä¼å¿…ç„¶è°ƒæ•´å¸‚åœºæˆ˜ç•¥ã€‚\\n',\n",
       "  'å› æ­¤åœŸåœ°å‚¨å¤‡è‡³å…³é‡è¦ã€‚\\n',\n",
       "  'ä¸­åŸåœ°äº§é¦–å¸­åˆ†æå¸ˆå¼ å¤§ä¼Ÿè¯´ã€‚\\n',\n",
       "  'ä¸€çº¿åŸå¸‚åœŸåœ°ä¾›åº”é‡å‡å°‘ã€‚\\n',\n",
       "  'ä¹ŸåŠ©æ¨äº†åœŸåœ°å¸‚åœºçš„ç«çˆ†ã€‚\\n',\n",
       "  'åŒ—äº¬å‡è–ªåƒ§ä½å®…åœŸåœ°ä¾›åº”æ—¶æ¾ã€‚\\n'],\n",
       " ['ç”šè‡³å‡ºç°äº¤æ˜“å‡ ä¹åœæ»çš„æƒ…å†µã€‚',\n",
       "  'ä¸€äºŒçº¿åŸå¸‚è™½ç„¶ä¹Ÿå¤„äºè°ƒæ•´ä¸­ã€‚',\n",
       "  'ä½†å› ä¸ºèšé›†äº†è¿‡å¤šå…¬å…±èµ„æºã€‚',\n",
       "  'ä¸ºäº†è§„é¿ä¸‰å››çº¿åŸå¸‚æ˜æ˜¾è¿‡å‰©çš„å¸‚åœºé£é™©ã€‚',\n",
       "  'æ ‡æ†æˆ¿ä¼å¿…ç„¶è°ƒæ•´å¸‚åœºæˆ˜ç•¥ã€‚',\n",
       "  'å› æ­¤åœŸåœ°å‚¨å¤‡è‡³å…³é‡è¦ã€‚',\n",
       "  'ä¸­åŸåœ°äº§é¦–å¸­åˆ†æå¸ˆå¼ å¤§ä¼Ÿè¯´ã€‚',\n",
       "  'ä¸€çº¿åŸå¸‚åœŸåœ°ä¾›åº”é‡å‡å°‘ã€‚',\n",
       "  'ä¹ŸåŠ©æ¨äº†åœŸåœ°å¸‚åœºçš„ç«çˆ†ã€‚',\n",
       "  'åŒ—äº¬ä»…æ–°å¢ä½å®…åœŸåœ°ä¾›åº”åå®—ã€‚'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0:10], record[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = zh_Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = text_tokenizer.batch_encode_plus(\n",
    "            label,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n",
    "record_ids = text_tokenizer.batch_encode_plus(\n",
    "            record,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7176, 40])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_ids['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label = text_tokenizer.batch_decode(\n",
    "                    label_ids['input_ids'], skip_special_tokens=True)\n",
    "decode_record = text_tokenizer.batch_decode(\n",
    "                    record_ids['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label  = [item.replace(' ', '') for item in decode_label]\n",
    "decode_record  = [item.replace(' ', '') for item in decode_record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_value = metric.compute(references=decode_label,predictions=decode_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04308167400016065"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value# å’Œè¿™é‡Œçš„å¾®å°å·®è·å¯èƒ½åœ¨ï¼Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ç”šè‡³å‡ºç°äº¤æ˜“å‡ ä¹åœæ»çš„æƒ…å†µã€‚',\n",
       " 'ä¸€äºŒçº¿åŸå¸‚è™½ç„¶ä¹Ÿå¤„äºè°ƒæ•´ä¸­ã€‚',\n",
       " 'å¤§å› ä¸ºèšé›†äº†è¿‡å¤šå…¬å…±èµ„æºã€‚',\n",
       " 'ä¸ºäº†è§„é¿ä¸‰å››çº¿åŸå¸‚æ˜æ˜¾è¿‡å‰©çš„å¸‚åœºé£é™©ã€‚',\n",
       " 'æ ‡æ†æˆ¿ä¼å¿…ç„¶è°ƒæ•´å¸‚åœºæˆ˜ç•¥ã€‚',\n",
       " 'å› æ­¤åœŸåœ°å‚¨å¤‡è‡³å…³é‡è¦ã€‚',\n",
       " 'ä¸­åŸåœ°äº§é¦–å¸­åˆ†æå¸ˆå¼ å¤§ä¼Ÿè¯´ã€‚',\n",
       " 'ä¸€çº¿åŸå¸‚åœŸåœ°ä¾›åº”é‡å‡å°‘ã€‚',\n",
       " 'ä¹ŸåŠ©æ¨äº†åœŸåœ°å¸‚åœºçš„ç«çˆ†ã€‚',\n",
       " 'åŒ—äº¬å‡è–ªåƒ§ä½å®…åœŸåœ°ä¾›åº”æ—¶æ¾ã€‚']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_label[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIDATATANG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/AIDATATANG/AIDATATANG_test.txt'\n",
    "with open(file_path, 'r') as f_zh_data:\n",
    "    data = f_zh_data.readlines()\n",
    "    utt = [item.split(' ')[0] for item in data]\n",
    "    label = [item.split(' ')[2] for item in data]\n",
    "    record = [item.split(' ')[1] for item in data]\n",
    "metric_value = metric.compute(references=label,predictions=record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06674709801510594"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['æœºç¥¨å¹¿å·æµå—ã€‚\\n',\n",
       "  'ç´¢é©¬é‡Œæµ·åŸŸåœ°ç†ä½ç½®é‡è¦æ€§ã€‚\\n',\n",
       "  'æ‰¾ä¸ªæ¿€æƒ…ç‰‡çœ‹ä¸€çœ‹ã€‚\\n',\n",
       "  'æ¥ä¸€éƒ¨å¥½çœ‹çš„ç”µå½±ã€‚\\n',\n",
       "  'æˆ‘è¦æ‹æ‘„äºŒç»´ç ã€‚\\n',\n",
       "  'æ’­æ”¾åŸæ¥ä½ ä»€ä¹ˆéƒ½ä¸æƒ³è¦ã€‚\\n',\n",
       "  'ä¸€æœˆåä¸€æ—¥ä¸Šåˆåå…«ç‚¹æé†’æˆ‘ã€‚\\n',\n",
       "  'ä½ ä¸æ˜¯è¦ç»™æˆ‘è®²ä¸ªç¬‘è¯å—ã€‚\\n',\n",
       "  'å®¶é‡Œé¢æœ‰ä¸€ä¸ªå°ç™½å…”ã€‚\\n',\n",
       "  'æ˜å¤©æœ‰æ²¡æœ‰é›¨å—ã€‚\\n'],\n",
       " ['æœºç¥¨å¹¿å·æµå—ã€‚',\n",
       "  'ç´¢é©¬é‡Œæµ·åŸŸåœ°ç†ä½ç½®é‡è¦æ€§ã€‚',\n",
       "  'æ‰¾ä¸ªæ¿€æƒ…ç‰‡çœ‹ä¸€çœ‹ã€‚',\n",
       "  'æ¥ä¸€éƒ¨å¥½çœ‹çš„ç”µå½±ã€‚',\n",
       "  'æˆ‘è¦æ‹æ‘„äºŒç»´ç ã€‚',\n",
       "  'æ’­æ”¾åŸæ¥ä½ ä»€ä¹ˆéƒ½ä¸æƒ³è¦ã€‚',\n",
       "  'ä¸€æœˆåä¸€æ—¥ä¸Šåˆåå…«ç‚¹æé†’æˆ‘ã€‚',\n",
       "  'ä½ ä¸æ˜¯è¦ç»™æˆ‘è®²ä¸ªç¬‘è¯å—ã€‚',\n",
       "  'å®¶é‡Œé¢æœ‰ä¸€ä¸ªå°ç™½å…”ã€‚',\n",
       "  'æ˜å¤©æœ‰æœ¨æœ‰é›¨å—ã€‚'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0:10], record[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = zh_Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = text_tokenizer.batch_encode_plus(\n",
    "            label,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n",
    "record_ids = text_tokenizer.batch_encode_plus(\n",
    "            record,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48144, 40])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_ids['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label = text_tokenizer.batch_decode(\n",
    "                    label_ids['input_ids'], skip_special_tokens=True)\n",
    "decode_record = text_tokenizer.batch_decode(\n",
    "                    record_ids['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label  = [item.replace(' ', '') for item in decode_label]\n",
    "decode_record  = [item.replace(' ', '') for item in decode_record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_value = metric.compute(references=decode_label,predictions=decode_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04308167400016065"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value# å’Œè¿™é‡Œçš„å¾®å°å·®è·å¯èƒ½åœ¨ï¼Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ç”šè‡³å‡ºç°äº¤æ˜“å‡ ä¹åœæ»çš„æƒ…å†µã€‚',\n",
       " 'ä¸€äºŒçº¿åŸå¸‚è™½ç„¶ä¹Ÿå¤„äºè°ƒæ•´ä¸­ã€‚',\n",
       " 'å¤§å› ä¸ºèšé›†äº†è¿‡å¤šå…¬å…±èµ„æºã€‚',\n",
       " 'ä¸ºäº†è§„é¿ä¸‰å››çº¿åŸå¸‚æ˜æ˜¾è¿‡å‰©çš„å¸‚åœºé£é™©ã€‚',\n",
       " 'æ ‡æ†æˆ¿ä¼å¿…ç„¶è°ƒæ•´å¸‚åœºæˆ˜ç•¥ã€‚',\n",
       " 'å› æ­¤åœŸåœ°å‚¨å¤‡è‡³å…³é‡è¦ã€‚',\n",
       " 'ä¸­åŸåœ°äº§é¦–å¸­åˆ†æå¸ˆå¼ å¤§ä¼Ÿè¯´ã€‚',\n",
       " 'ä¸€çº¿åŸå¸‚åœŸåœ°ä¾›åº”é‡å‡å°‘ã€‚',\n",
       " 'ä¹ŸåŠ©æ¨äº†åœŸåœ°å¸‚åœºçš„ç«çˆ†ã€‚',\n",
       " 'åŒ—äº¬å‡è–ªåƒ§ä½å®…åœŸåœ°ä¾›åº”æ—¶æ¾ã€‚']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAGICDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/users/jiangjin/jiangjin_bupt/ASR_CORRECTION/Cross_modal/TAP/data/zh/MAGICDATA/MAGICDATA_test.txt'\n",
    "with open(file_path, 'r') as f_zh_data:\n",
    "    data = f_zh_data.readlines()\n",
    "    utt = [item.split(' ')[0] for item in data]\n",
    "    label = [item.split(' ')[2] for item in data]\n",
    "    record = [item.split(' ')[1] for item in data]\n",
    "metric_value = metric.compute(references=label,predictions=record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07398781866368245"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['å£å£éŸ³ä¹ã€‚\\n',\n",
       "  'æµ·å¤©æ°”æµ·å†·è®°å¾—å¤©è¡£ä¿æš–å“¦ã€‚\\n',\n",
       "  'äººçš„èƒ½è€å†å¤§å¤§ä¸è¿‡å¤©è¿™é›¨è½¬é˜´åˆå˜äº†å¤©è€å¤©è¦ä¸‹é›¨å¨˜è¦å«äººç”±å¥¹å»å§ã€‚\\n',\n",
       "  'æ’­æ”¾äºŒèƒ¡ç‹¬å¥ã€‚\\n',\n",
       "  'å¯æ˜¯åˆ®é£äº†ã€‚\\n',\n",
       "  'å—æ´‹æŒç»­é™é›¨æ°´æ·±è·¯æ»‘å‡ºè¡Œä¸€å®šè¦æ³¨æ„åŸƒä¸”åˆšå›å®¶çš„è·¯ä¸Šæˆ‘å·®ç‚¹è¢«å†²è¿›ä¸‹æ°´é“ã€‚\\n',\n",
       "  'æ’­æ”¾é›ªè‰çš„æ­Œæ›²ã€‚\\n',\n",
       "  'æˆ‘æƒ³è¿™å‡ºæœ€å¥½æ­Œæ›²æŠŠæ­Œè¯å‘åˆ°ç½‘ä¸Šè¯·åˆ«äººå¸®æˆ‘ä½œæ›²æ¥æœºã€‚\\n',\n",
       "  'åˆ«è¯´ä¸ªå¤§æ¦‚é½è¯´å…·ä½“æ˜¯å“ªé¦–æ­Œã€‚\\n',\n",
       "  'é™¤äº†è‡ªå‘½ä¸ç™½å’Œçˆ±æ˜¯ä¸€ç§å¹¸ç¦ã€‚\\n'],\n",
       " ['å£å£éŸ³ä¹ã€‚',\n",
       "  'å—¨å¤©æ°”å¯’å†·è®°å¾—æ·»è¡£ä¿æš–å“¦ã€‚',\n",
       "  'äººçš„èƒ½è€å†å¤§å¤§ä¸è¿‡å¤©é˜µé›¨è½¬é˜´åˆå˜äº†å¤©äº†å¤©è¦ä¸‹é›¨å¨˜è¦å«äººç”±å¥¹å»å§ã€‚',\n",
       "  'æ’­æ”¾äºŒèƒ¡ç‹¬å¥ã€‚',\n",
       "  'å¯æ˜¯åˆ®é£äº†ã€‚',\n",
       "  'å—é˜³æŒç»­é™é›¨æ°´æ·±è·¯æ»‘å‡ºè¡Œä¸€å®šè¦æ³¨æ„å®‰å…¨åˆšå›å®¶çš„è·¯ä¸Šæˆ‘å·®ç‚¹å„¿è¢«å†²è¿›ä¸‹æ°´é“ã€‚',\n",
       "  'æ’­æ”¾é›ªè‰çš„æ­Œæ›²ã€‚',\n",
       "  'æˆ‘æƒ³è¿™å‡ºæœ€å¥½æ­Œæ›²æŠŠæ­Œè¯å‘åˆ°ç½‘ä¸Šè¯·åˆ«äººå¸®æˆ‘ä½œæ›²æ€¥æ€¥ã€‚',\n",
       "  'åˆ«è¯´ä¸ªå¤§æ¦‚é½è¯´å…·ä½“æ˜¯å“ªé¦–æ­Œã€‚',\n",
       "  'é™¤äº†è‡ªå‘½ä¸å‡¡å’Œçˆ±æ˜¯ä¸€ç§å¹¸ç¦ã€‚'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0:10], record[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = zh_Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = text_tokenizer.batch_encode_plus(\n",
    "            label,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n",
    "record_ids = text_tokenizer.batch_encode_plus(\n",
    "            record,\n",
    "            # max_length=self.config.max_seq_length,\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24279, 74])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_ids['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label = text_tokenizer.batch_decode(\n",
    "                    label_ids['input_ids'], skip_special_tokens=True)\n",
    "decode_record = text_tokenizer.batch_decode(\n",
    "                    record_ids['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label  = [item.replace(' ', '') for item in decode_label]\n",
    "decode_record  = [item.replace(' ', '') for item in decode_record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_value = metric.compute(references=decode_label,predictions=decode_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07398837811577272"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_value# å’Œè¿™é‡Œçš„å¾®å°å·®è·å¯èƒ½åœ¨ï¼Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['å£å£éŸ³ä¹ã€‚',\n",
       " 'æµ·å¤©æ°”æµ·å†·è®°å¾—å¤©è¡£ä¿æš–å“¦ã€‚',\n",
       " 'äººçš„èƒ½è€å†å¤§å¤§ä¸è¿‡å¤©è¿™é›¨è½¬é˜´åˆå˜äº†å¤©è€å¤©è¦ä¸‹é›¨å¨˜è¦å«äººç”±å¥¹å»å§ã€‚',\n",
       " 'æ’­æ”¾äºŒèƒ¡ç‹¬å¥ã€‚',\n",
       " 'å¯æ˜¯åˆ®é£äº†ã€‚',\n",
       " 'å—æ´‹æŒç»­é™é›¨æ°´æ·±è·¯æ»‘å‡ºè¡Œä¸€å®šè¦æ³¨æ„åŸƒä¸”åˆšå›å®¶çš„è·¯ä¸Šæˆ‘å·®ç‚¹è¢«å†²è¿›ä¸‹æ°´é“ã€‚',\n",
       " 'æ’­æ”¾é›ªè‰çš„æ­Œæ›²ã€‚',\n",
       " 'æˆ‘æƒ³è¿™å‡ºæœ€å¥½æ­Œæ›²æŠŠæ­Œè¯å‘åˆ°ç½‘ä¸Šè¯·åˆ«äººå¸®æˆ‘ä½œæ›²æ¥æœºã€‚',\n",
       " 'åˆ«è¯´ä¸ªå¤§æ¦‚é½è¯´å…·ä½“æ˜¯å“ªé¦–æ­Œã€‚',\n",
       " 'é™¤äº†è‡ªå‘½ä¸ç™½å’Œçˆ±æ˜¯ä¸€ç§å¹¸ç¦ã€‚']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3228b2b04fe574edc1cfad377da45b01c8bfafeb5d7448f83b83b0b5984135"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
